---
title: "Journal (reproducible report)"
author: "Lionel Will"
date: "2020-11-23"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    code_folding: hide
    message: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```


# Section 1: Intro to the tidyverse

Last compiled: `r Sys.Date()`

Our Task was to analyze the given bike sales data. After previously analyzing the data by the bike-type and manufacturing-year we now want to take a look at the number of sales sorted by the state in which they were sold in. Afterwards we want to take a look how these numbers changed over the last years.

Let's first take a look at the number of sales per state if you add up all the previous years together:

```{r}
# 1.0 Load libraries ----
  library(tidyverse)
  library(lubridate)
  library(writexl)
  #Read Excel Files
  library(readxl)

# 2.0 Importing Files ----
  bikes <- read_xlsx(path = "~/Documents/GitHub/DS_101/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
  bike_shops <- read_xlsx(path = "~/Documents/GitHub/DS_101/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")
  bike_orders <- read_xlsx(path = "~/Documents/GitHub/DS_101/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")

# 3.0 Examining Data ----
  glimpse(bike_orders)

# 4.0 Joining Data ----
  bike_orderlines_joined <- bike_orders %>% left_join(bikes, by = c("product.id"="bike.id")) %>% left_join(bike_shops, by = c("customer.id" = "bikeshop.id"))
  bike_orderlines_joined %>% glimpse()
  #glimpse(bike_orderlines_joined)
  
# 5.0 Wrangling Data ----
  bike_orderlines_joined %>% 
  select(category) %>%
  filter(str_detect(category, "^Mountain")) %>% 
  unique()

# 6.0 Business Insights ----
  #Separate the joined category category into 3 separate columns
  bike_orderlines_joined <- separate(bike_orderlines_joined, col=category, into = c("cat.1","cat.2","cat.3"), sep = " - ")
  #Calculate the total price of the order
  bike_orderlines_joined <- mutate(bike_orderlines_joined, total.price = quantity*price)
  
  bike_orderlines_wrangled <- bike_orderlines_joined
  
  #8.1 Which state sells the most of the bikes?
  bike_orderlines_bystate <- bike_orderlines_wrangled

  #Separate the joined location category into 2 separate columns
  bike_orderlines_bystate <- separate(bike_orderlines_wrangled, col=location, into = c("state","city"), sep = ", ")

  #State and City Variables are switched :D
  # Manipulate the data
  sales_by_state <- bike_orderlines_bystate
  
  # Select columns
  sales_by_state %>% 
  select(state, city, quantity,total.price) %>% 
    
  # Grouping by year and summarizing sales
  group_by(city) %>% 
  summarize(numberofsales = sum(total.price)) %>%

  # Step 2 - Visualize
    
  # Setup canvas with the columns year (x-axis) and sales (y-axis)
  ggplot(aes(x = city, y = numberofsales)) +
    
  # Geometries
  geom_col(fill = "#2DC6D6") + # Use geom_col for a bar plot
  geom_label(aes(label = numberofsales), labels = dollar_format(suffix = "€", prefix = "", decimal.mark = ",", big.mark = ".")) + # Adding labels to the bars
  geom_smooth(method = "lm", se = FALSE) + # Adding a trendline
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    
  # Formatting
  scale_y_continuous(labels = dollar_format(suffix = "€", prefix = "", decimal.mark = ",", big.mark = ".")) + # Change the y-axis. 
  # Again, we have to adjust it for euro values
  labs(
      title    = "Revenue by state",
      subtitle = "NRW has the most bike sales in the last years",
      x = "", # Override defaults for x and y
      y = "Revenue"
    )
```

Interesting! Let's see if NRW is still the state with the most sales if we additionally sort them by their manufacturing-year:


```{r plot, fig.width= 10, fig.height=5}
 #8.2 Which state sells the most of the bikes in which year?
  sales_by_stateandyear <- bike_orderlines_bystate
  
  # Select columns
  sales_by_stateandyear %>% 
  select(state, city, quantity, order.date) %>% 
  mutate(year = year(order.date)) %>%
    
  # Group by and summarize year and main catgegory
  group_by(state,city,year) %>%
  summarise(numberofsales_2 = sum(quantity)) %>%
  ungroup() %>% 
    
  # Set up x, y, fill
  ggplot(aes(x = year, y = numberofsales_2 ))+ #, fill = numberofsales_2 )) +
    
  # Geometries
  geom_col() + # Run up to here to get a stacked bar plot
    
  # Facet
  facet_wrap(~ city) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
      title = "Revenue by year and state",
      subtitle = "Each product category has an upward trend",
      fill = "Main category" # Changes the legend name
    )
 
```

# Section 2: Data Aquisition
Let's gather some data from the international space station. Maybe the astronauts can help with the bike sales all over the country. Who doesn't want to have their spectacular view over the world?
```{r}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
library(httr)
library(glue)

Current_astronauts_raw <- GET("http://api.open-notify.org/astros.json")
stop_for_status(Current_astronauts_raw) # automatically throws an error if a request did not succeed
Current_astronauts <- fromJSON(rawToChar(Current_astronauts_raw$content)) 
Current_astronauts <- Current_astronauts$people %>% 
                      select(name,craft)
Current_astronauts
```

Hm that was to be expected. There are currently a few people on the ISS but I suspect they have better things to do than to help with bike sales. They can't even use them up there :D. Now let's shift back to the bike sales in order to determine the bike categories of the competition because we need to improve the sales data from our company by ourself:
```{r}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
library(httr)
library(glue)

#8.2 Get the the model names, categories and prices of the bikes from rosebikes.de
# 8.2.1 COLLECT PRODUCT FAMILIES ----
  url_home          <- "https://www.rosebikes.de/fahrräder" 

  # Read in the HTML for the entire webpage
  html_home         <- read_html(url_home) 

  # Web scrape the ids for the families
  bike_family_cop <- html_home %>% 
  
  # Get the nodes for the families ...
  html_nodes(css = ".catalog-navigation__link") %>%
  # ...and extract the information of the id attribute
  html_attr('title') %>% 
    
  # Remove the product families SALE and BIKE-FINDER 
  discard(.p = ~stringr::str_detect(.x,"Sale|Bike-Finder")) %>%
    
  # Convert vector to tibble effectivly giving each entry a number
  enframe(name = "position", value = "family_class") %>%
    
  # Add a hashtag so we can get nodes of the categories by id (#)
  mutate(
    family_id = str_glue("#{family_class}")
  )
    
#8.2.2 COLLECT PRODUCT CATEGORIES URL ----
  
  # Combine all Ids to one string so that we will get all nodes at once
  # (seperated by the OR operator ",")
  #family_id_css <- bike_family_cop %>%
  #  pull(family_id) %>%
  #  stringr::str_c(collapse = ", ")
  
  # Extract the urls from the href attribute
  bike_category_cop <- html_home %>%
    
  # Select nodes by the ids
  html_nodes(css = ".catalog-navigation__link") %>%
  html_attr('href') %>%
  
  # Remove the product families SALE and BIKE-FINDER 
  discard(.p = ~stringr::str_detect(.x,"sale|zoovu")) %>%  
    
  # Convert vector to tibble
  enframe(name = "position", value = "subdirectory") %>%
    
  # Add the domain, because we will get only the subdirectories
  mutate(
    url = glue("https://www.rosebikes.de{subdirectory}")
  ) %>%
    
  # Some categories are listed multiple times.
  # We only need unique values
  distinct(url)
  
#8.2.3 GET THE MODEL NAMES FROM THE URLs ----
    #First Category
    # select first bike category url
    bike_model_url1 <- bike_category_cop$url[1]
  
    # Get the URLs for the bikes of the first category
    html_bike_model_cop1  <- read_html(bike_model_url1) %>% 
  
    # Select nodes by the ids
    html_nodes(css = ".catalog-category-bikes__title") %>%
    html_text('catalog-category-bikes__title-text') %>%  
    
    # Convert vector to tibble
    enframe(name = "position", value = "name")
    
    #2nd Category
    # select first bike category url
    bike_model_url2 <- bike_category_cop$url[2]
    
    # Get the URLs for the bikes of the first category
    html_bike_model_cop2  <- read_html(bike_model_url2) %>% 
      
    # Select nodes by the ids
    html_nodes(css = ".catalog-category-bikes__title") %>%
    html_text('catalog-category-bikes__title-text') %>%  
      
    # Convert vector to tibble
    enframe(name = "position", value = "name")    
  
#8.2.4 GET THE MODEL PRICES FROM THE URLs ----
  #First Category
  # select first bike category url
  bike_model_url1 <- bike_category_cop$url[1]
  
  # Get the URLs for the bikes of the first category
  html_bike_model_price_cop1  <- read_html(bike_model_url1) %>% 
      
  # Select nodes by the ids
  html_nodes(css = ".catalog-category-bikes__price") %>%
  html_text('catalog-category-bikes__price-title') %>%  
    
  # Remove the query parameters of the URL (everything after the '?')
  str_remove(pattern = "ab ") %>%
  str_remove(pattern = "oder ab.*") %>%
    
  # Convert vector to tibble
  enframe(name = "position", value = "name")
    
  #2nd Category
  # select first bike category url
  bike_model_url2 <- bike_category_cop$url[2]
    
  # Get the URLs for the bikes of the first category
  html_bike_model_price_cop2  <- read_html(bike_model_url2) %>% 
      
  # Select nodes by the ids
  html_nodes(css = ".catalog-category-bikes__price") %>%
  html_text('catalog-category-bikes__price-title') %>%  
  
  # Remove the query parameters of the URL (everything after the '?')
  str_remove(pattern = "ab ") %>%
  str_remove(pattern = "oder ab.*") %>%
  
  # Convert vector to tibble
  enframe(name = "position", value = "name")    
  
  #Join the vectors together
  bike_cat1_cop <- left_join(html_bike_model_cop1,html_bike_model_price_cop1, by = c("position"="position"))
  bike_cat2_cop <- left_join(html_bike_model_cop2,html_bike_model_price_cop2, by = c("position"="position"))
  
  
  bike_category_cop
  bike_cat1_cop
  bike_cat2_cop
```


# Section 3: Data Wrangling
Let's use the data from the USPTO website to find out a little bit about patents in the US. 

First let's take a look at the TOP 10 US companies in regards to the number of patents:
```{r}
#10 Challenge ----
library(vroom)
library(tidyverse)
library(lubridate)
library(writexl)
#Read Excel Files
library(readxl)
library(data.table)

#10.1. First Part: Which US company has the most patents? List the 10 US companies with the most granted patents ----

col_types_assignee <- list(
  id = col_character(),
  type = col_character(),
  nn = col_character(),
  nn2 = col_character(),
  organization = col_character()
)

patent_assignee <- vroom(
  file       = "~/Dokumente (Offline)/assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_assignee),
  col_types  = col_types_assignee,
  na         = c("", "NA", "NULL")
) %>% 
  select(id, type, organization)
  patent_assignee <- patent_assignee[-c(1), ]

col_types_p_assignee <- list(
  patent_id = col_character(),
  assignee_id = col_character()
)

patent_p_assignee <- vroom(
  file       = "~/Dokumente (Offline)/patent_assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_p_assignee),
  col_types  = col_types_p_assignee,
  na         = c("", "NA", "NULL")
)%>% 
  select(patent_id, assignee_id)
  patent_p_assignee <- patent_p_assignee[-c(1), ]

combined_patent_assignee_data <- left_join(patent_assignee,patent_p_assignee, by = c("id" = "assignee_id"))

combined_patent_assignee_data_DT <- as.data.table(combined_patent_assignee_data) %>% 
    filter((type == 2)) %>% 
    group_by(organization) %>% 
    summarise(total = n()) %>% 
    ungroup() %>% 
    arrange(desc(total))
head(combined_patent_assignee_data_DT,10)
```
Now let's take a look at the TOP 10 US companies in 2019 in regards to their number of filed patents:
```{r}
#10 Challenge ----
library(vroom)
library(tidyverse)
library(lubridate)
library(writexl)
#Read Excel Files
library(readxl)
library(data.table)
#10.2 What US company had the most patents granted in 2019? List the top 10 companies of 2019 ----

col_types_assignee <- list(
  id = col_character(),
  type = col_character(),
  nn = col_character(),
  nn2 = col_character(),
  organization = col_character()
)

patent_assignee <- vroom(
  file       = "~/Dokumente (Offline)/assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_assignee),
  col_types  = col_types_assignee,
  na         = c("", "NA", "NULL")
) %>% 
  select(id, type, organization)
patent_assignee <- patent_assignee[-c(1), ]

col_types_p_assignee <- list(
  patent_id = col_character(),
  assignee_id = col_character()
)

patent_p_assignee <- vroom(
  file       = "~/Dokumente (Offline)/patent_assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_p_assignee),
  col_types  = col_types_p_assignee,
  na         = c("", "NA", "NULL")
)%>% 
  select(patent_id, assignee_id)
patent_p_assignee <- patent_p_assignee[-c(1), ]

col_types_patent <- list(
  patent_id = col_character(),
  pat_type = col_character(),
  pat_number = col_character(),
  pat_country = col_character(),
  pat_date = col_date("%Y-%m-%d")
)

patent_list <- vroom(
  file       = "~/Dokumente (Offline)/patent.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_patent),
  col_types  = col_types_patent,
  na         = c("", "NA", "NULL")
  ) %>% 
  select(patent_id, pat_date)
patent_list <- patent_list[-c(1), ]

combined_patent_data_prev <- left_join(patent_assignee,patent_p_assignee, by = c("id" = "assignee_id"))
combined_patent_data <- left_join(combined_patent_data_prev,patent_list, by = c("patent_id"="patent_id"))

combined_patent_data_DT <- as.data.table(combined_patent_data) %>% 
  mutate(pat_date = year(pat_date)) %>% 
  filter((type == 2)) %>% 
  filter((pat_date == 2019)) %>% 
  group_by(organization) %>% 
  summarise(total = n()) %>% 
  ungroup() %>% 
  arrange(desc(total))
head(combined_patent_data_DT,10)
```
Lastly let's take a look at their USPTO tech main classes. What are the TOP 5 tech main classes of the TOP 10 companies filing patents in the US worldwide. 
```{r}
#10 Challenge ----
library(vroom)
library(tidyverse)
library(lubridate)
library(writexl)
#Read Excel Files
library(readxl)
library(data.table)


col_types_assignee <- list(
  id = col_character(),
  type = col_character(),
  nn = col_character(),
  nn2 = col_character(),
  organization = col_character()
)

patent_assignee <- vroom(
  file       = "~/Dokumente (Offline)/assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_assignee),
  col_types  = col_types_assignee,
  na         = c("", "NA", "NULL")
) %>% 
  select(id, type, organization)
patent_assignee <- patent_assignee[-c(1), ]

col_types_p_assignee <- list(
  patent_id = col_character(),
  assignee_id = col_character()
)

patent_p_assignee <- vroom(
  file       = "~/Dokumente (Offline)/patent_assignee.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_p_assignee),
  col_types  = col_types_p_assignee,
  na         = c("", "NA", "NULL")
)%>% 
  select(patent_id, assignee_id)
patent_p_assignee <- patent_p_assignee[-c(1), ]

col_types_uspc <- list(
  nn3 = col_character(),
  patent_id = col_character(),
  mainclass_id = col_character()
)

patent_uspc <- vroom(
  file       = "~/Dokumente (Offline)/uspc.tsv", 
  delim      = "\t", 
  col_names  = names(col_types_uspc),
  col_types  = col_types_uspc,
  na         = c("", "NA", "NULL")
) %>% 
  select(patent_id,mainclass_id)
patent_uspc <- patent_uspc[-c(1), ]

combined_uspc_data_prev <- left_join(patent_assignee,patent_p_assignee, by = c("id" = "assignee_id"))
combined_uspc_data <- left_join(combined_uspc_data_prev,patent_uspc, by = c("patent_id"="patent_id"))


combined_uspc_data_companies_DT <- as.data.table(combined_uspc_data) %>% 
  group_by(organization) %>% 
  summarise(total = n()) %>% 
  ungroup() %>% 
  arrange(desc(total))
combined_uspc_data_companies_DT <- combined_uspc_data_companies_DT[-c(2), ]
combined_uspc_data_companies_DT <- head(combined_uspc_data_companies_DT,10)
combined_uspc_data_companies_DT

combined_uspc_patentdata_DT <- as.data.table(combined_uspc_data) %>% 
  group_by(organization) %>% 
  select(organization,mainclass_id) %>% 
  filter((organization == combined_uspc_data_companies_DT[1,1]|organization == combined_uspc_data_companies_DT[2,1]|organization == combined_uspc_data_companies_DT[3,1]|organization == combined_uspc_data_companies_DT[4,1]|organization == combined_uspc_data_companies_DT[5,1]|organization == combined_uspc_data_companies_DT[6,1]|organization == combined_uspc_data_companies_DT[7,1]|organization == combined_uspc_data_companies_DT[8,1]|organization == combined_uspc_data_companies_DT[9,1]|organization == combined_uspc_data_companies_DT[10,1])) %>% 
  group_by(mainclass_id) %>% 
  summarise(total = n()) %>% 
  ungroup() %>% 
  arrange(desc(total))
combined_uspc_patentdata_DT <- combined_uspc_patentdata_DT[-c(1), ]
head(combined_uspc_patentdata_DT,5)
```

# Section 4: Data Visualization
As the last part, let's take a look at our more recent global situation. How did the COVID-19 case number evolve throughout the year 2020?
```{r}
library(tidyverse)
library(lubridate)
library(writexl)
#Read Excel Files
library(readxl)
library(data.table)
library(scales)
library(ggrepel)
library(maps)

# 1.1 COVID data preparation ----
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")


covid_data_tbl_DT <- as.data.table(covid_data_tbl) %>% 
  filter(countriesAndTerritories == "Germany"|countriesAndTerritories == "France"|countriesAndTerritories == "Spain"|countriesAndTerritories == "United_Kingdom"|countriesAndTerritories == "United_States_of_America") %>%  
  select("dateRep", "countriesAndTerritories", cases) %>%
  mutate(dateRep=dmy(dateRep)) %>%
  arrange(dateRep, desc()) %>% 
  group_by(countriesAndTerritories) %>% 
    mutate(cum_cases = cumsum(cases)) %>% 
  ungroup()

Cases_France <- covid_data_tbl_DT %>% 
  filter(countriesAndTerritories == "France") %>% 
  select(dateRep,cum_cases) %>% 
  as.data.table()

Cases_Spain <- covid_data_tbl_DT %>% 
  filter(countriesAndTerritories == "Spain") %>% 
  select(dateRep,cum_cases) %>% 
  as.data.table()

Cases_Germany <- covid_data_tbl_DT %>% 
  filter(countriesAndTerritories == "Germany") %>% 
  select(dateRep,cum_cases) %>% 
  as.data.table()

Cases_UK <- covid_data_tbl_DT %>% 
  filter(countriesAndTerritories == "United_Kingdom") %>% 
  select(dateRep,cum_cases) %>% 
  as.data.table()

Cases_Europe <- left_join(Cases_France, Cases_Spain, by = c("dateRep" = "dateRep"))
Cases_Europe <- left_join(Cases_Europe, Cases_Germany, by = c("dateRep" = "dateRep"))
Cases_Europe <- left_join(Cases_Europe, Cases_UK, by = c("dateRep" = "dateRep")) %>% 
                    add_column("Europe",.after = 1) %>%
                    rename("France"  = "cum_cases.x",
                        "Spain" = "cum_cases.y",
                        "Germany" = "cum_cases.x.x" ,
                        "UK" = "cum_cases.y.y",
                          "countriesAndTerritories" = 2) %>% 
                    mutate(cum_cases = France+Spain+Germany+UK) %>% 
                    select(dateRep,countriesAndTerritories,cum_cases) %>% 
                    as.data.table()
covid_data_tbl_DT <- covid_data_tbl_DT%>% 
  select(dateRep,countriesAndTerritories, cum_cases)
  
covid_data_tbl_DT <- rbind(covid_data_tbl_DT,Cases_Europe) %>% 
  arrange(dateRep, desc())

#1.2 COVID data plot
covid_data_tbl_DT %>%
# Canvas
ggplot(aes(x = dateRep,y = cum_cases, color = countriesAndTerritories))+
# Geometries 
geom_line(size = 0.5, linetype = 1, aes(color=countriesAndTerritories)) + 
expand_limits(y = 0) +

labs(
  title = "COVID-19 confirmed cases worldwide",
  subtitle = "As of 03/12/2020. Europe only consists of countries shown with the execption of the USA.",
  x = "Year 2020",
  y = "Cumulatitve Cases"
)+   

theme_light()+
theme(legend.position  = "bottom", 
      legend.direction = "vertical") +
guides(col = guide_legend(ncol = 3,
       title = "Continent / Country",
       title.position = "left"
     )
  )+
scale_color_manual(values=c('grey50','red','blue','darkgreen','brown','orange'))+
scale_x_date(labels = date_format(("%b")),breaks = breaks_width("1 month")) +
scale_y_continuous(labels = scales::dollar_format(scale = 1/1000000, 
                                                    prefix = "",
                                                    suffix = " M")) +

geom_label_repel(aes(
  label=ifelse((dateRep == "2020-12-02" & countriesAndTerritories == "United_States_of_America") , as.character(cum_cases),'')), 
  nudge_x = -10,
  label.padding = unit(0.55, "lines"), # Rectangle size around label
  label.size = 0.35,
  point.padding = unit(0.55,"lines"),
  segment.color = 'orange',
  color = "white",
  fill="orange"
)+

geom_label_repel(aes(
  label=ifelse((dateRep == "2020-12-01" & countriesAndTerritories == "Europe") , as.character(cum_cases),'')), 
  nudge_x = -40,
  label.padding = unit(0.55, "lines"), # Rectangle size around label
  label.size = 0.35,
  point.padding = unit(0.55,"lines"),
  segment.color = 'grey50',
  color = "white",
  fill="grey50"
)
```

And which countries have the most death cases due to the corona virus in regards to their population?
```{r}
library(tidyverse)
library(lubridate)
library(writexl)
#Read Excel Files
library(readxl)
library(data.table)
library(scales)
library(ggrepel)
library(maps)


worldmap_raw <- map_data("world")

# 1.1 COVID data preparation ----
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")

covid_data_tbl_DT2 <- covid_data_tbl %>% 
  mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
  mutate(countriesAndTerritories = case_when(
    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories
  ))

covid_data_tbl_DT2 <- covid_data_tbl_DT2 %>% 
  select(dateRep, deaths, popData2019, countriesAndTerritories) %>% 
  group_by(countriesAndTerritories) %>% 
  mutate(dateRep=dmy(dateRep)) %>%
  arrange(dateRep, desc()) %>% 
  mutate(cum_deaths = cumsum(deaths)) %>% 
  ungroup() %>% 
  filter(dateRep == "2020-11-25") %>%
  mutate(relative_deaths = (cum_deaths/popData2019)) %>% 
  select(countriesAndTerritories, relative_deaths)

worldmap <- worldmap_raw %>%
  select(long,lat,group,region)
worldmap_COVID <- left_join(worldmap, covid_data_tbl_DT2, by = c("region" = "countriesAndTerritories")) %>% 
mutate(relative_deaths_rounded = round(relative_deaths,4))

ggplot(worldmap_COVID, aes(map_id = region))+
  geom_map(aes(fill = relative_deaths_rounded), map = worldmap_COVID) +
  expand_limits(x = worldmap_COVID$long, y=worldmap_COVID$lat)+

  
labs(
  title = "Confirmed COVID-19 deaths relative to the size of the population",
  subtitle = "More than 1.2 Million confirmed COVID-19 deaths worldwide",
  x="",
  y=""
)+
  
theme_light()+
theme(legend.position  = "right", 
      legend.direction = "vertical",
      axis.text.x = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks = element_blank()) +
scale_fill_continuous(name = "Mortality Rate", type = "viridis")#, labels = scales::percent(scale = 1, decimal.mark = ".", suffix = " %"))
```


